# Introduction #

This Page will give the details and the procedure adopted for Producing Fully labelled Meshes in a variety of Poses

It will also guide the user on how to use the source code generated.


# Details #

Ok, so before you do anything please checkout a copy of the repository from this Project Page. (Instructions in Tutuorial if time permits)

## Step One ##

The very first step in development was the making of a BVH Reader that could read the entire CMU Mocap Database and consequently parse it to find poses that have maximum joint separation of greater then 5cm between two consecutive poses.


<table>
<tr>
<td>
<img width='320' height='240' src='http://opencv-kinect.googlecode.com/svn/wiki/Images/PoseDifference1.jpg' />
</td>
<td>
<img width='320' height='240' src='http://opencv-kinect.googlecode.com/svn/wiki/Images/PoseDifference2.jpg' />
</td>
</tr>
</table>

The BVH Reader is provided along with a sample program that was used to remove the redundantPoses file by file. Here again a smarter trick was played and the poses were parsed file by file rather then globally. This gives the advantage of recognizing More Often Occuring Poses as they will occur in the parsed poses of multiple files and hence will automatically be given a greater weightage in training. (Smart? I think so)

If this does not suit you there are function in the repository you can use to remove this duplication. Just use ExploreDirectories to get a listing of all BVH Files and read the lines used to generate the poses into a vector String, use the adoptPose to the Mesh to the Pose and compare against all previous poses. (Too Memory Intensive as the size grows)

It has a function called 'uniquePoseIdentification()' that can be used to output a bvh file which contains unique poses only!

Now the BVH Reader Can be used to generate a view of the Pose. This is simply a lines and circles representation that is stored in cv::Mat. And
it looks something like this.

<img width='320' height='240' src='http://opencv-kinect.googlecode.com/svn/wiki/Images/BVHReader.jpg' />

The mainprogram of the bvhReader is currently set to displaying all the contents of the BVH File it is set to.

Now open the Explore Module of the code and run it to obtain a simple listing of all the paths to the BVH files that were generated by the BVH  Reader.This will be saved as 'PathsToFiles.txt' This file will be used in the step two of this procedure.


## Step Two ##

In the second step we wanted to project a mesh (a character) that we designed and whose body features we could easily vary into these poses.

This is where the whole Labelling Pipeline began and this is how it works

1- Download MakeHuman from [Download Page](https://sites.google.com/site/makehumandocs/download|MakeHuman)

2- Download Blender from [Download Page](http://www.blender.org/download/get-blender/|Blender)

3- Install both of these!

4- Next Use MakeHuman to make a character of your choice.

5- Export that character in the following formats '.obj' and '.mhx' and also save the makeHuman file for that character

6- This is saved in documents/makehuman/exports and documents/makehuman/models and respectively

7- Next open Blender and first import the .obj file with Blender (REMEMBER TO KEEP THE GROUPS OPTION CHECKED!)

8- Now delete the cube , the camera and the lights from the scene and run the script from /Code/Graphics\_Pipeline/BlenderScripts/ called 'BrokenMesh.py'

You should read the intro section of this script as you will have to specify a path where you want to output the files. It will generate four files in a folder you specify

9- Next first make changes to the settings for impoting the mhx rig as stated over http://makehuman.blogspot.com/2011/08/recently-i-have-been-working-on-new.html|Here

10- Now import the MHX file and delete the clothes from within the character Object and the cube and the Camera and the Lights from the World.

11- Now run the script from /Code/Graphics\_Pipeline/BlenderScripts/ called 'MHXInitializing.py'. Once again please read the intro instructions and set the path to the same folder Preferrably.

12- Now Run the sample program titled 'mainprogram.cpp' in the Labelling Module. Please read the comments and adjust the paths before you run it.
This will generate Labels for the MHX file.

13- Now Run the file from /Code/Graphics\_Pipeline/BlenderScripts/ called 'GeneratePoseData.py'. Once again, please read the intro instructions very carefully as you will have to specify the path to the 'PathsToFiles.txt' that you generated in the first phase and also where you want to place your output files.

[ABOUT THE SCRIPT: GeneratePoseData.py uses 'loadRetargetSimplify' a script from the MakeHuman Repository that seems to have a memory Leak. Hence it is only able to output 650 poses on my machine. However, i have plans to Fix this in a future release if allowed by GSOC](NOTE.md)


## Step Three ##

You can now open the Display module and specify the path t0

1- the Labels file generated from substep 12 of the previous Step. (it is automatically generated on program termination as a precaution to loosing data as the script takes quite some time to execute)

2- the faces file that is output by the 'MHXInitializing.py'

3- for the coordinates file use the Explore Directories to obtain the paths to all the .coords files in the directory


Now use the list of files for coordinates specified to one by one load the coordinates and render them :D

### Things Missed From Step Three ###
**It is intended to load the rendering into a cv::Mat** It is intended to load the rendering from the depth buffer into a cv::Mat


## The Much Intended and Awaited and Attempted to Reach Step Four that was Never Reached ##
**Training the randomised Forests**


